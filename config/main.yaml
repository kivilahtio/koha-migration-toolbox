---

##
## IN THIS FILE
##
## The main configurations for MMT Voyager
##
##
## Configurations are introduced in the same order as they are needed by the migration tools

# From which system are we migrating from?
# One of ['Voyager', 'PrettyLib', 'PrettyCirc']
sourceSystemType: 'Voyager'

############################
### 1) Data export phase ###
## Voyager DB dumps are fetched and delivered for the data transformation phase

# Name of the script in directory $MMT_HOME/secret used to extract data from the VoyagerDB-server into the Koha-server.
# See the extractor-dir for example implementations.
exportPipelineScript: 'voyagerToTransformer.sh'

####################################
### 2) Data transformation phase ###
## Exported DB rows are mangled into Koha-objects

# Anonymize Borrower records?
anonymize: 1

# Enable multithreading by defining workers.
workers: 0

# Load ssns to Hetula instead of storing them in Koha as plain text.
# Boolean value
useHetula: false

# Holdings transformation strategy to use.
# The way holdings records are used in various databases vary, hence one transformation set of rules wont work for all source databases.
holdingsTransformationModule: HAMK

# If the item doesn't have a price or a replacementprice, use this default.
# Use '' (empty String) to skip setting the default replacement price
defaultReplacementPrice: 25

# ISIL-code for the migrated organization
# Used primarily in the MARC record transformations.
organizationISILCode: FI-Hamk

# Set the Patron name initials to F, L (where F = firstname, L = lastname)
patronInitials: 0

# Set the Patron home library to this value for all patrons. Leave empty to use the LocationId-translation table.
patronHomeLibrary:

# How many expiry years to add for patron account if expiry date is not defined in Voyager
patronAddExpiryYears: 0

# Are patron data anonymized by default in Koha?
# 2 - never save privacy information. Koha tries to save as little info as possible
# 1 - Default
# 0 - Gather and keep data about me!
patronDefaultPrivacy: 1

# Which strategy to use to validate phone numbers
# See MMT::Validator::Phone for available strategies
phoneNumberValidationStrategy: HAMK

# How many expiry days to add for reserves if expiry date is not defined in PrettyLib
# TODO: Voyager support
reserveAddExpiryDays: 365

# How to deal with missing barcodes?
# ERROR: Barcodes must exist, mocks a barcode based on the Id
# IGNORE: Barcodes might be missing
# CREATE: Barcodes must exist. Creates a barcode using the pattern 'emptyBarcodePattern'
emptyBarcodePolicy: IGNORE

# Replaces zeroes (0) with the id of the object being transformed
emptyBarcodePattern: HAMK00000000

# Minimum length for a barcode.
# If barcode doesn't fit here, a new barcode is generated.
# TODO: Voyager support
barcodeMinLength: 5

# Apply a set of regular expression replaces into barcode
# TODO: Voyager support
# e.g.
#barcodeRegexReplace:
#  - regex: '[ÄÅ]'
#    replace: A
#  - regex: 'Ö'
#    replace: O
barcodeRegexReplace: []

# How Perl's Text::CSV should handle .csv-files input?
# Output is hard-coded and should not be changed to maintain zero-configuration compatibility with the Loader-modules
csvInputParams:
  new:
    binary: 1
    sep_char: ","
    auto_diag: 9
    always_quote: 1
  header:
    detect_bom: 1
    munge_column_names: none
  encoding: ISO-8859-1

# How are the MARC-files encoded? To see the OS supported encodings, use command:
#   $ perl -e 'use Encode; @a=Encode->encodings(); print "@a\n"'
marcInputEncoding: utf-8

# Adds new repeatable field(s) to MARC record containing statically defined data below:
# e.g. 
#marcAddCustomRepeatableField:
#  - tag: 540
#    indicator1: '#'
#    indicator2: '#'
#    subfields:
#      - 3: Metadata
#        a: Ei poimintaa. No copying;
#        b: Org
marcAddCustomRepeatableField: []

############ PRETTY* -specific configs ################

# Instead of the Item.BarCode use Item.AcqNumIdx to populate koha.items.barcode
pl_barcodeFromAcqNumber: 0

# Use these regexps to classify PrettyLib/Circ.Class-table's ClassText-column to different ontologies.
# if one of the regexp matches, the classification is put to the given MARC21 field with the extra static subfield contents
pl_class_classifiers:
  - regexp: ".*"
    field: 084
    indicator1: '#'
    indicator2: '#'
    subfields:
      - q: FI-Hamk

# Drop all Item Shelf (itemcallunmber) values that match this filter.
# eg. "(?:nu|es|ki)"
pl_shelf_filter: ""

# MARC21 Format for bibliographic records recommends to define Field 490 as the primary Series statement for a series title.
# And 90X-83X - Series Added Entry Fields can supplement that.
# This supplementation looks a bit weird in Koha, as the same title/statement is shown both for the 490 and 8XX.
# In Koha, the default Koha-to-MARC-mappings dictate: biblio.seriestitle => 490$a, so it is better to use "490" here if using both fields is not wanted.
# One can generate:
# "490" = only the Field 490, losign detail from PrettyLib regarding the distinction between fields 800, 810, 811, 830
# "8XX" = 8XX + FinMARC conversion rules generate 490 too.
pl_biblio_seriesMARCCompatibility: 490


#############################
### 3) Data loading phase ###
## Exported DB rows are mangled into Koha-objects

# Name of the script in directory $MMT_HOME/secret used to import the transformed data into Koha
# See the importer-dir for example implementations.
importPipelineScript: 'transformerToKoha.sh'

##
## Migration chain processing hooks
##

# After processing Koha.Item.itemcallnumber-field content, run this custom code subroutine within the MMT::$sourceSystemType::Item -package
# eg. custom_itemcallnumber_PV_hopea
# set to 0 to disable
Item_setItemcallnumber_posthook: 0

# Author-table entries are filtered with this Perl regexp. MARC subfields are extracted from the text. All named capture groups must map to MARC subfields.
# eg.    s/(?<e>\(toim\.\))//    - All Author-fields that contain (toim.), have the matched text removed and put into subfield $e
# eg.    s/(?<relatorterm)\(toim.\)//   - Trigger special relator-term handling to populate 700$x and 100$e from regex matches.
Biblio_authorFilter: 0

# Matches from Biblio_authorFilter are translated with this key-value map
# e.g.
# "toim.": toimittaja
Biblio_authorFilterMatchTranslationMap:

# International Standard Numbers (ISBN, ISSN, ISRC, ISMN, ISRN) are filtered with this Perl regexp.
Biblio_ISBNFilter: '(?=[0-9X]{10}|(?=(?:[0-9]+[- ]){3})[- 0-9X]{13}|97[89][0-9]{10}|(?=(?:[0-9]+[- ]){4})[- 0-9]{17})(?:97[89][- ]?)?[0-9]{1,5}[- ]?[0-9]+[- ]?[0-9]+[- ]?[0-9X]'
Biblio_ISSNFilter: '[0-9]{4}-[0-9]{3}[0-9xX]'
Biblio_ISRCFilter: '[A-Z]{2}[- ]?\w{3}[- ]?\d{2}[- ]?\d{5}'
Biblio_ISMNFilter: '(?:979-?0-?|M-?)(?:\d{9}|(?=[\d-]{11}$)\d+-\d+-\d)'
Biblio_ISRNFilter: '^.{1,36}'
